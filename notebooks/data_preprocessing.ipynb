{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac99dcb0",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING\n",
    "Data preprocessing is the key step in any data analysis or machine learning pipeline. This step often interleaved with **Data Exploration**. It involves cleaning, transforming and organizing raw data to ensure it is accurate, consistent and ready for modeling. It has a big impact on model building such as:\n",
    "\n",
    "- Clean and well-structured data allows models to learn meaningful patterns rather than noise.\n",
    "\n",
    "- Properly processed data prevents misleading inputs, leading to more reliable predictions.\n",
    "\n",
    "- Organized data makes it simpler to create useful inputs for the model, enhancing model performance.\n",
    "\n",
    "- Organized data supports better Exploratory Data Analysis (EDA), making patterns and trends more interpretable.\n",
    "\n",
    "**Steps in Data Preprocessing:** Some key steps in data preprocessing are: **Data Cleaning**, **Data Reduction**, **Feature Engineering** and **Data Splitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be24b9",
   "metadata": {},
   "source": [
    "## I. Data Cleaning\n",
    "It is the process of correcting errors or inconsistencies, normalization and handling any missing, duplicated or irrelevant data in the dataset which was discovered by us through step **Data Exploration**. Clean data is essential for effective analysis, as it improves the quality of results and enhances the performance of data models. We will seperate data cleaning into **six key steps**, included in:\n",
    "\n",
    "- **Synstax Errors**: Correcting issues such as **typos, incorrect data types, and invalid characters**.\n",
    "\n",
    "- **Format Data**: Standardizing data formats, including **dates, numeric values, text cases, and units of measurement**.\n",
    "\n",
    "- **Irrelevant Data**: Removing data fields or records that do not contribute to the analytical objective.\n",
    "\n",
    "- **Handling Missing Data & Impossible Values**: Addressing missing values or abnormal values through **removal, imputation, or estimation**.\n",
    "\n",
    "- **Handling Duplicated Data**: Eliminating duplicate records.\n",
    "\n",
    "- **Handling Outlier**: Managing extreme which is detected.\n",
    "\n",
    "Through insights gained from **Data Understanding of the Raw Dataset**, we recognize that the collected data is relatively clean, with no duplicated and wrong format. Therefore, in this section, we only focus on addressing the remaining issues through necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0a9ca",
   "metadata": {},
   "source": [
    "At the beginning like any step, we also need import the libraries as well as load the data by `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9ef47",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458ae945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src import preprocessing as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09648d12",
   "metadata": {},
   "source": [
    "**Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06a61d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS]: Loading dataset successful\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/raw/vietnam_air_quality.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"[SUCCESS]: Loading dataset successful\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR]: Loading dataset fail: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c22be4",
   "metadata": {},
   "source": [
    "### 1. Format Data\n",
    "As we explored in step **explore datatype of the data** at **Data Exploration**. Most of columns have been right datatype except column `timestamp`. Therefore, in this section we will convert it to right datatype that is `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfe4885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Formating data is successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='raise')\n",
    "    print(\"[SUCCESS] Formating data is successful\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Formating data fail: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7ef3e",
   "metadata": {},
   "source": [
    "### 2. Irrelevant Data\n",
    "The goal of this step is reduce memory and noise by removing columns which is not essential for analysis as well as build model.\n",
    "\n",
    "With **20 columns** in our dataset, we will decide to drop column `lat`, `lon`  because the data from the above column was used for mapping to create the **`city`** column.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204cb41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Irrelevant column is successfull\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.drop(columns=[\"lat\", \"lon\"], inplace=True)\n",
    "    print(\"[SUCCESS] Irrelevant column is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Irrelevant column is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011a76b",
   "metadata": {},
   "source": [
    "### 3. Handling Missing Data & Impossible Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd61a6e",
   "metadata": {},
   "source": [
    "- As identified in the **Data Exploration** phase, **93 records** violated the physical constraint where fine particulate matter ($PM_{2.5}$) exceeded coarse particulate matter ($PM_{10}$). We addressed these anomalies using a three-step imputation strategy: \n",
    "    - **Masking:** The invalid $PM_{10}$ values in these specific rows were converted to NaN (treated as missing). \n",
    "    - **Interpolation:** We applied **Time-based Interpolation** to reconstruct the missing values. Crucially, this operation was grouped by city to ensure spatial independence and preserve local time-series trends. \n",
    "    - **Constraint Enforcement:** A final consistency check was applied to strictly enforce $PM_{10} \\geq PM_{2.5}$, ensuring physical validity across the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7472850",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_invalid = df[df[\"pm2_5\"] > df[\"pm10\"]].index\n",
    "df.loc[mask_invalid, \"pm10\"] = np.nan\n",
    "\n",
    "# Set timestamp as Index (Required for time interpolation)\n",
    "df = df.set_index(\"timestamp\")\n",
    "\n",
    "# Interpolate\n",
    "df[\"pm10\"] = df.groupby(\"city\")[\"pm10\"].transform(\n",
    "    lambda group: group.interpolate(method=\"time\", limit=5)\n",
    ")\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index()\n",
    "df[\"pm10\"] = df[[\"pm10\", \"pm2_5\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de04956",
   "metadata": {},
   "source": [
    "- **Checking After Resolve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9805e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-fix Verification: Found 0 impossible rows (PM2.5 > PM10).\n"
     ]
    }
   ],
   "source": [
    "# Check remaining PM2.5 > PM10 violations\n",
    "remaining_invalid = df[df[\"pm2_5\"] > df[\"pm10\"]].index\n",
    "print(f\"Post-fix Verification: Found {len(remaining_invalid)} impossible rows (PM2.5 > PM10).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12634e",
   "metadata": {},
   "source": [
    "### 4. Handling Outlier\n",
    "Before jumping into handle outlier we need save processed data for explore continuously which we process in previous step. This data is ready to define about distribution data and from this we can have insight about outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177eb33",
   "metadata": {},
   "source": [
    "**Save Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943c84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed/processed_data.csv\"\n",
    "df.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca07dc1",
   "metadata": {},
   "source": [
    "Based on the insights derived from the `data_exploration.ipynb` notebook and the physical characteristics of air quality data, we have decided **NOT to remove or alter statistical outliers** (extreme high values). These are some reason about it:\n",
    "\n",
    "1. **Authentic Environmental Variability vs. Error**\n",
    "   - **Insight:** As noted in the **Data Exploration** phase, the \"outliers\" (e.g., AQI > 150) represent periods of **severe air pollution events**.\n",
    "   - **Reasoning:** In environmental science, extreme values are often \"features,\" not \"bugs.\" A value of `AQI = 146` (as seen in *Bac Ninh*) is a realistic occurrence during winter or temperature inversion periods. Removing these values would artificially smooth the data, causing it to deviate from reality.\n",
    "\n",
    "2. **Preservation of Time-Series Continuity**\n",
    "    - **Constraint:** The data is a continuous hourly time series.\n",
    "    - **Reasoning:** Deep learning models (e.g., LSTM, GRU) and feature engineering techniques (e.g., Lag features $t-1$, Rolling Windows) require an unbroken temporal sequence. Deleting rows containing outliers would create **time gaps**, disrupting the temporal dependencies required for accurate forecasting.\n",
    "\n",
    "3. **Multivariate Integrity**\n",
    "   - **Logic:** Pollution levels are physically correlated with meteorological factors (e.g., high PM2.5 often correlates with low `wind_speed` or high `humidity`).\n",
    "   - **Reasoning:** Removing a high PM2.5 data point while retaining its corresponding weather conditions destroys the physical cause-and-effect relationship. The model needs these extreme examples to learn how specific weather patterns trigger pollution spikes.\n",
    "\n",
    "\n",
    "However, if in step **Modeling** the accurancy or efficiency of model doesn't good performance we will consider to fall back in this step - **Handling Outliers**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f4246",
   "metadata": {},
   "source": [
    "## II. Data Reduction\n",
    "The method of **Data Reduction** may achieve a condensed description of the original data which is **much smaller in quantity but keeps the quality** of the original data. After **step EDA**, with `processed_data` we continue to remove some columns inclued in: `aqi`, `pollution_level` and `pollution_class` which we have analyzed through step **Data Undestanding about Processed Data**. Now, these columns aren't essential with data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ff3e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Irrelevant column is successfull\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = [\"aqi\", \"pollution_level\", \"pollution_class\"]\n",
    "try:\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(\"[SUCCESS] Irrelevant column is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Irrelevant column is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82209eec",
   "metadata": {},
   "source": [
    "## III. Feature Engineering\n",
    "\n",
    "**Feature Engineering** is the process of selecting, scaling, transforming, or creating new features from raw data. It helps turn messy real-world data into a form that the model can understand, thereby improving its predictive performance.\n",
    "\n",
    "At the **EDA (Data Exploration)** step, we initially identified several potential features based on domain knowledge. We revisit and refine them here because these features are hypothesized to significantly enhance regression performance. Additionally, **for time-series problems**, we specifically engineer variables to capture **temporal dependencies**, **seasonality**, and **lag effects**—critical components for forecasting accuracy.\n",
    "\n",
    "We have categorized the feature extraction process into **4 main groups**, based on the physical and statistical nature of the data:\n",
    "\n",
    "1. **Group 1: Temporal & Social Context**: To capture natural time cycles (diurnal/seasonal) and human behavioral patterns (rush hours, holidays) that influence emission levels.\n",
    "\n",
    "- **Cyclical Features:** `hour_sin`, `hour_cos`, `month_sin`, `month_cos`.\n",
    "  - *Significance:* These transform time variables (0-23 hours, 1-12 months) into sine/cosine coordinates, allowing the model to understand continuity (e.g., 23:00 is temporally close to 00:00).\n",
    "\n",
    "- **Calendar Features:** `hour`, `day_of_week`, `month`, `season`.\n",
    "\n",
    "- **Social Activity:**\n",
    "  - `is_weekend`: Distinguishes between weekdays and weekends (reflecting differences in traffic flow).\n",
    "  \n",
    "  - `is_working_day`, `is_rush_hour`: Marks peak traffic hours and workdays, where vehicle emissions typically spike.\n",
    "  \n",
    "  - `day_part`: Segments the day into periods (Morning, Noon, Afternoon, Evening, Night) to group behavioral characteristics.\n",
    "\n",
    "  **Impact:** Enables the model to learn **recurrent patterns** based on Hour, Day, and Season.\n",
    "\n",
    "2. **Group 2: Physics & Meteorology:** To model the physical impact of weather conditions on the dispersion, accumulation, or cleansing of air pollutants.\n",
    "\n",
    "- **Wind Vectors:** `wind_x`, `wind_y`.\n",
    "  - *Significance:* Decomposes wind direction and speed into two orthogonal vectors. This handles directionality better than degrees (where 0° and 360° are physically identical but numerically distant).\n",
    "\n",
    "- **Washout Effect:** `rain_sum_6h`.\n",
    "  - *Significance:* Cumulative rainfall over the last 6 hours, reflecting the atmosphere's \"cleansing\" capacity (rain washout).\n",
    "\n",
    "- **Thermodynamics:**\n",
    "    * `temp_diff_24h`: Diurnal temperature range, influencing thermal inversion phenomena.\n",
    "    \n",
    "    * `humid_x_temp`: Interaction term between humidity and temperature (e.g., high humidity + low temp often leads to fog/haze, trapping pollutants).\n",
    "\n",
    "  **Impact:** Reflects environmental **physical mechanisms**: Wind aids dispersion, rain cleanses the air, while specific temperature/humidity conditions facilitate secondary pollutant formation.\n",
    "\n",
    "3. **Group 3: History & Trend (PM2.5):** To exploit the strong **autocorrelation** inherent in time-series data. Current pollution levels are highly dependent on the immediate past.\n",
    "\n",
    "- **Lag Features:** `pm25_lag_1h`, `pm25_lag_2h`, `pm25_lag_3h`, `pm25_lag_24h`.\n",
    "  - *Significance:* Direct PM2.5 values at previous time steps (1 hour ago, 1 day ago).\n",
    "\n",
    "- **Rolling Statistics:**\n",
    "  - `pm25_rm_6h`, `pm25_rm_24h`: Rolling means over 6h and 24h, representing the **background trend**.\n",
    "  \n",
    "  - `pm25_rs_6h`: Rolling standard deviation, representing data **volatility**.\n",
    "\n",
    "- **Trend:** `pm25_trend_1h` (Instantaneous change compared to the previous hour).\n",
    "\n",
    "  **Impact:** This is empirically the **most critical feature group**, contributing most significantly to the regression model's predictive performance.\n",
    "\n",
    "4. **Group 4: Composition & Exogenous Variables:** To leverage correlations between PM2.5 and other pollutants (beneficial multicollinearity) and analyze pollution composition.\n",
    "\n",
    "- **Dust Composition:**\n",
    "  - `coarse_dust` ($PM_{10} - PM_{2.5}$): Represents the coarse particle fraction.\n",
    "  \n",
    "  - `pm_ratio` ($PM_{2.5} / PM_{10}$): The ratio of fine dust.\n",
    "  \n",
    "  - `coarse_dust_lag1h`, `pm_ratio_lag1h`.\n",
    "\n",
    "- **Gas Pollutant Lags:** `no2_lag1h`, `so2_lag1h`, `co_lag1h`, `o3_lag1h`.\n",
    "  - *Significance:* Uses past concentrations of precursor gases to predict PM2.5, as emission sources (vehicles, factories) typically release both gases and particulates simultaneously.\n",
    "\n",
    "  **Impact:** Helps the model distinguish between pollution types (fine vs. coarse dust events) and utilizes predictive signals from chemical precursors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563432c8",
   "metadata": {},
   "source": [
    "**Replicate Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c31f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clone to avoid effect on processed data\n",
    "df_feat = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc7132",
   "metadata": {},
   "source": [
    "**Group 1: Temporal & Social Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2455ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Group 1: Temporal & Social...\n",
      "[SUCCESS] Creating features is successfull\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_feat = dp.create_feature_temporal_social(df_feat)\n",
    "    print(\"[SUCCESS] Creating features is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Creating features is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ebc9b",
   "metadata": {},
   "source": [
    "**Group 2: Physics & Meteorology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11fc3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Group 2: Physics & Meteo...\n",
      "[SUCCESS] Creating features is successfull\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_feat = dp.create_feature_physic(df_feat)\n",
    "    print(\"[SUCCESS] Creating features is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Creating features is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f0ee4",
   "metadata": {},
   "source": [
    "**Group 3: History & Trend (PM2.5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f32483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Group 3: History & Trend...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Creating features is successfull\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_feat = dp.create_feature_history_trend(df_feat)\n",
    "    print(\"[SUCCESS] Creating features is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Creating features is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f50a3da",
   "metadata": {},
   "source": [
    "**Group 4: Composition & Exogenous Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81df149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Group 4: Composition...\n",
      "[SUCCESS] Creating features is successfull\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_feat = dp.create_feature_composition(df_feat)\n",
    "    print(\"[SUCCESS] Creating features is successfull\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Creating features is fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e81f35",
   "metadata": {},
   "source": [
    "## IV. Data Spliting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef73def",
   "metadata": {},
   "source": [
    "To prepare the dataset for supervised learning, we construct a **one-hour-ahead forecasting target** by shifting the PM2.5 concentration forward by one time step within each city. Rows containing missing values introduced by shifting and feature engineering are subsequently removed to ensure data consistency.\n",
    "\n",
    "The cleaned dataset is then split **chronologically** into three subsets to prevent temporal leakage:\n",
    "\n",
    "- **Training set**: observations before *August 1, 2025*\n",
    "- **Validation set**: observations from *August 1, 2025* to *September 30, 2025*\n",
    "- **Test set**: observations from *October 1, 2025* onward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6d7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (718896, 48), from start to 2025-06-01 00:00:00\n",
      "Validation set: (99552, 48), from 2025-06-01 00:00:00 to 2025-10-01 00:00:00\n",
      "Test set: (98736, 48), from 2025-10-01 00:00:00 to end\n"
     ]
    }
   ],
   "source": [
    "train, val, test, train_cut, val_cut = dp.train_val_test_split(df_feat)\n",
    "\n",
    "print(f\"Train set: {train.shape}, from start to {train_cut}\")\n",
    "print(f\"Validation set: {val.shape}, from {train_cut} to {val_cut}\")\n",
    "print(f\"Test set: {test.shape}, from {val_cut} to end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ce01f",
   "metadata": {},
   "source": [
    "## Data Persistence\n",
    "This step serves as the “bridge” between the Preprocessing step and the Modeling step. After we preprocess data ready for training, we need to save this data for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92c1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/model\"\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "train.to_csv(os.path.join(path, \"train.csv\"), index = False)\n",
    "val.to_csv(os.path.join(path, \"val.csv\"), index = False)\n",
    "test.to_csv(os.path.join(path, \"test.csv\"), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
